{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple CNN\n",
    "This notebook trains a simplified version of the GoogLeNet model. # Simple CNN"
   ],
   "id": "795677b8ba7a0209"
  },
  {
   "cell_type": "code",
   "id": "b5d2de5c2b9075a6",
   "metadata": {},
   "source": [
    "# Set to True if you want to run Tensorflow on a GPU.\n",
    "use_gpu = False\n",
    "\n",
    "if not use_gpu:\n",
    "    print(\"Installing Tensorflow with CPU support...\")\n",
    "    !pip install tensorflow\n",
    "else:\n",
    "    print(\"Installing Tensorflow with GPU support...\")\n",
    "    !pip install tensorflow[and-cuda]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install matplotlib",
   "id": "e18b8f3c581da8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ef8b35f-637c-4703-990b-753bcbe06941",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If using the GPU, setting this to True will cause Tensorflow to not allocate all available memory. This will prevent the GPU from running out of memory at the cost of training speed.\n",
    "enable_memory_growth = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        print(f\"Found {len(gpus)} GPU(s), setting memory growth to {enable_memory_growth}\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, enable_memory_growth)"
   ],
   "id": "2c70512da1bfddfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ingesting the input images\n",
    "\n",
    "### Prepare the ImageNet images\n",
    "In the same directory as this notebook, you will find a file named `imagenet_224.zip`. This zip file contains the ImageNet images used for training the CNN.\n",
    "Unzip the files to a folder named `imagenet_224`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "imagenet_224\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "e3c90db6fa066ab5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMAGE_DIR = os.path.join(os.getcwd(), \"imagenet_224\")\n",
    "\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{IMAGE_DIR} not found. You need to download the ImageNet dataset and unzip it into a folder called imagenet_224\")\n",
    "\n",
    "# You can adjust the batch size depending on the compute resources\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "TARGET_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    IMAGE_DIR,           \n",
    "    validation_split=0.2,     \n",
    "    subset=\"training\",          \n",
    "    seed=111,                    \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE              \n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    IMAGE_DIR,          \n",
    "    validation_split=0.2,       \n",
    "    subset=\"validation\",        \n",
    "    seed=111,                    \n",
    "    image_size=TARGET_SIZE,     \n",
    "    batch_size=BATCH_SIZE              \n",
    ")"
   ],
   "id": "aafc18d2529bb5ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance enhancements\n",
    "Apply augmentation and prefetching to improve training performance."
   ],
   "id": "b5228a0b6b81714c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Apply the augmentation only on the training dataset\n",
    "train_dataset = train_dataset.map(lambda x, y: (augmentation(x, training=True), y))\n",
    "\n",
    "# Prefetch the datasets for performance improvement\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "id": "f4f850c14606b198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the CNN",
   "id": "ced49b3c8b8f69fc"
  },
  {
   "cell_type": "code",
   "id": "6fc92fc52ffb8497",
   "metadata": {},
   "source": [
    "# There are 10 image classes.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Specify a lower learning late because of the small data set.\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Add the layers using the Functional API\n",
    "inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = layers.Conv2D(32, (7, 7), strides=(2, 2), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "9e75016a7061fef0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can adjust the number of epochs as needed.\n",
    "EPOCHS = 30\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,                        \n",
    "    validation_data=validation_dataset\n",
    ")"
   ],
   "id": "14b5dbea75947606",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save training artifacts",
   "id": "62a248c0cd61b9b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model to a file.\n",
    "model.save(os.path.join(os.getcwd(), \"model_googlenet_simple.keras\"))\n",
    "\n",
    "# Save the history to a file.\n",
    "with open(os.path.join(os.getcwd(), \"history_googlenet_simple.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(history, file)"
   ],
   "id": "3ad17b2c847f24c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assess the model's performance",
   "id": "25fdfd45f327f475"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot the accuracy curve",
   "id": "d6d8d573dc0fab44"
  },
  {
   "cell_type": "code",
   "id": "cdb1ca57def88917",
   "metadata": {},
   "source": [
    "def plot_accuracy_curve(training_result, metric):\n",
    "    val_metric = f\"val_{metric}\"\n",
    "    train_perf = training_result.history[metric]\n",
    "    validation_perf = training_result.history[val_metric]\n",
    "    \n",
    "    plt.plot(train_perf, label=metric)\n",
    "    plt.plot(validation_perf, label=val_metric)\n",
    "    \n",
    "    max_val = max(validation_perf)\n",
    "    max_val_epoch = validation_perf.index(max_val)\n",
    "    \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "plot_accuracy_curve(history, \"accuracy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find the epoch at which the difference in training and validation accuracies are minimized.",
   "id": "31de90d2834bc8fc"
  },
  {
   "cell_type": "code",
   "id": "9ca54507-c9a1-4202-b09b-bb1837b4fb97",
   "metadata": {},
   "source": [
    "train_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "acc_diff = [abs(train - val) for train, val in zip(train_acc, val_acc)]\n",
    "\n",
    "min_diff = min(acc_diff)\n",
    "min_diff_epoch = acc_diff.index(min_diff) + 1\n",
    "\n",
    "train_acc_at_min_diff = train_acc[min_diff_epoch - 1]  \n",
    "val_acc_at_min_diff = val_acc[min_diff_epoch - 1]      \n",
    "\n",
    "print(f\"Minimum difference between accuracy and validation accuracy: {min_diff:.1f} at epoch {min_diff_epoch}\")\n",
    "print(f\"Training Accuracy at epoch {min_diff_epoch}: {train_acc_at_min_diff:.1f}\")\n",
    "print(f\"Validation Accuracy at epoch {min_diff_epoch}: {val_acc_at_min_diff:.1f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the model on unseen ImageNet images.\n",
    "\n",
    "### Prepare the evaluation ImageNet images\n",
    "In the same directory as this notebook, you will find a file named `imagenet_224_eval.zip`. This zip file contains the ImageNet images used for evaluating the CNN.\n",
    "Unzip the files to a folder named `imagenet_224_eval`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "imagenet_224_eval\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "9e7baa54c0a65fa9"
  },
  {
   "cell_type": "code",
   "id": "e4bf4f3f-2675-449e-aeae-8259f01d6111",
   "metadata": {},
   "source": [
    "UNSEEN_IMAGENET_IMG_DIR = os.path.join(os.getcwd(), \"imagenet_224_eval\")\n",
    "\n",
    "if not os.path.exists(UNSEEN_IMAGENET_IMG_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{UNSEEN_IMAGENET_IMG_DIR} not found. You need to download the ImageNet evaluation dataset and unzip it into a folder called imagenet_224_eval\")\n",
    "\n",
    "eval_imagenet_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    UNSEEN_IMAGENET_IMG_DIR,          \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE             \n",
    ")\n",
    "\n",
    "# Prefetch for better performance.\n",
    "eval_imagenet_dataset = eval_imagenet_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "loss, accuracy = model.evaluate(eval_imagenet_dataset)\n",
    "\n",
    "print(f\"Loss on unseen ImageNet images: {loss:.1f}\")\n",
    "print(f\"Accuracy on unseen ImageNet images: {accuracy:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the model on CIFAR-10 images.\n",
    "\n",
    "### Prepare the evaluation CIFAR-10 images\n",
    "In the same directory as this notebook, you will find a file named `cifar-10.zip`. This zip file contains the ImageNet images used for evaluating the CNN against a totally different dataset - CIFAR-10.\n",
    "Unzip the files to a folder named `cifar-10`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "cifar-10\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "b55f2c9a8960e762"
  },
  {
   "cell_type": "code",
   "id": "f7ca01ed-cbdc-48f5-8783-8d5fc2084b99",
   "metadata": {},
   "source": [
    "CIFAR10_IMG_DIR = os.path.join(os.getcwd(), \"cifar-10\")\n",
    "\n",
    "if not os.path.exists(CIFAR10_IMG_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{CIFAR10_IMG_DIR} not found. You need to download the CIFAR-10 dataset and unzip it into a folder called cifar-10\")\n",
    "\n",
    "eval_cifar_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    CIFAR10_IMG_DIR,           \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE              \n",
    ")\n",
    "\n",
    "# Prefetch for better performance.\n",
    "eval_cifar_dataset = eval_cifar_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "loss, accuracy = model.evaluate(eval_cifar_dataset)\n",
    "\n",
    "print(f\"Loss on CIFAR-10 images: {loss:.1f}\")\n",
    "print(f\"Accuracy CIFAR-10 images: {accuracy:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbb33138b685dad7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
