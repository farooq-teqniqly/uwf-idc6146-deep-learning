{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GoogLeNet CNN Reproduction\n",
    "This notebook trains a reproduction of the complex GoogLeNet model."
   ],
   "id": "795677b8ba7a0209"
  },
  {
   "cell_type": "code",
   "id": "b5d2de5c2b9075a6",
   "metadata": {},
   "source": [
    "# Set to True if you want to run Tensorflow on a GPU.\n",
    "use_gpu = False\n",
    "\n",
    "if not use_gpu:\n",
    "    print(\"Installing Tensorflow with CPU support...\")\n",
    "    !pip install tensorflow\n",
    "else:\n",
    "    print(\"Installing Tensorflow with GPU support...\")\n",
    "    !pip install tensorflow[and-cuda]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install matplotlib",
   "id": "e18b8f3c581da8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ef8b35f-637c-4703-990b-753bcbe06941",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If using the GPU, setting this to True will cause Tensorflow to not allocate all available memory. This will prevent the GPU from running out of memory at the cost of training speed.\n",
    "enable_memory_growth = False\n",
    "\n",
    "if use_gpu:\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        print(f\"Found {len(gpus)} GPU(s), setting memory growth to {enable_memory_growth}\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, enable_memory_growth)"
   ],
   "id": "2c70512da1bfddfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ingesting the input images\n",
    "\n",
    "### Prepare the ImageNet images\n",
    "In the same directory as this notebook, you will find a file named `imagenet_224.zip`. This zip file contains the ImageNet images used for training the CNN.\n",
    "Unzip the files to a folder named `imagenet_224`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "imagenet_224\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "e3c90db6fa066ab5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMAGE_DIR = os.path.join(os.getcwd(), \"imagenet_224\")\n",
    "\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{IMAGE_DIR} not found. You need to download the ImageNet dataset and unzip it into a folder called imagenet_224\")\n",
    "\n",
    "# You can adjust the batch size depending on the compute resources\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "TARGET_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    IMAGE_DIR,           \n",
    "    validation_split=0.2,     \n",
    "    subset=\"training\",          \n",
    "    seed=111,                    \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE              \n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    IMAGE_DIR,          \n",
    "    validation_split=0.2,       \n",
    "    subset=\"validation\",        \n",
    "    seed=111,                    \n",
    "    image_size=TARGET_SIZE,     \n",
    "    batch_size=BATCH_SIZE              \n",
    ")"
   ],
   "id": "aafc18d2529bb5ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance enhancements\n",
    "Apply augmentation and prefetching to improve training performance."
   ],
   "id": "b5228a0b6b81714c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Apply the augmentation only on the training dataset\n",
    "train_dataset = train_dataset.map(lambda x, y: (augmentation(x, training=True), y))\n",
    "\n",
    "# Prefetch the datasets for performance improvement\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "id": "f4f850c14606b198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the CNN",
   "id": "ced49b3c8b8f69fc"
  },
  {
   "cell_type": "code",
   "id": "6fc92fc52ffb8497",
   "metadata": {},
   "source": [
    "# There are 10 image classes.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Specify a lower learning late because of the small data set.\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "def inception_layer(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj):\n",
    "    \"\"\"\n",
    "    Creates an Inception layer that applies multiple convolutions and pooling operations in parallel and concatenates the results.\n",
    "\n",
    "    The Inception layer performs the following steps in parallel:\n",
    "    1. A 1x1 convolution with 'filters_1x1' filters.\n",
    "    2. A 1x1 convolution followed by a 3x3 convolution, where 'filters_3x3_reduce' controls the number of filters in the initial 1x1 convolution, \n",
    "       and 'filters_3x3' specifies the number of filters for the 3x3 convolution.\n",
    "    3. A 1x1 convolution followed by a 5x5 convolution, where 'filters_5x5_reduce' controls the number of filters in the initial 1x1 convolution,\n",
    "       and 'filters_5x5' specifies the number of filters for the 5x5 convolution.\n",
    "    4. A 3x3 max pooling operation followed by a 1x1 convolution with 'filters_pool_proj' filters.\n",
    "\n",
    "    Finally, the outputs of all these parallel operations are concatenated along the channel axis to form the final output.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor to the Inception layer.\n",
    "        filters_1x1 (int): Number of filters for the 1x1 convolution path.\n",
    "        filters_3x3_reduce (int): Number of filters for the 1x1 convolution before the 3x3 convolution.\n",
    "        filters_3x3 (int): Number of filters for the 3x3 convolution.\n",
    "        filters_5x5_reduce (int): Number of filters for the 1x1 convolution before the 5x5 convolution.\n",
    "        filters_5x5 (int): Number of filters for the 5x5 convolution.\n",
    "        filters_pool_proj (int): Number of filters for the 1x1 convolution after the max pooling.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Output tensor resulting from concatenating the filters' outputs.\n",
    "    \"\"\"\n",
    "    conv_1x1 = layers.Conv2D(filters_1x1, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    \n",
    "    conv_3x3 = layers.Conv2D(filters_3x3_reduce, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    conv_3x3 = layers.Conv2D(filters_3x3, (3, 3), padding=\"same\", activation=\"relu\")(conv_3x3)\n",
    "    \n",
    "    conv_5x5 = layers.Conv2D(filters_5x5_reduce, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    conv_5x5 = layers.Conv2D(filters_5x5, (5, 5), padding=\"same\", activation=\"relu\")(conv_5x5)\n",
    "    \n",
    "    max_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    max_pool = layers.Conv2D(filters_pool_proj, (1, 1), padding=\"same\", activation=\"relu\")(max_pool)\n",
    "    \n",
    "    output = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, max_pool], axis=-1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define the full model\n",
    "def googlenet(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Builds a GoogLeNet Inception model.\n",
    "\n",
    "    The model consists of the following components:\n",
    "    1. Initial convolution and max-pooling layers.\n",
    "    2. Two main convolutional blocks, each followed by max-pooling layers.\n",
    "    3. Multiple Inception layers that apply parallel convolutional and pooling operations,\n",
    "       followed by max-pooling layers between certain inception layers.\n",
    "    4. Global Average Pooling.\n",
    "    5. Dropout to prevent overfitting.\n",
    "    6. A final fully connected layer with a softmax activation function.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input image.\n",
    "        num_classes (int): Number of output classes for classification.\n",
    "    \n",
    "    Returns:\n",
    "        keras.Model: A Keras Model instance representing the GoogLeNet architecture.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    \n",
    "    x = layers.Conv2D(192, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    \n",
    "    x = inception_layer(x, filters_1x1=64, filters_3x3_reduce=96, filters_3x3=128,\n",
    "                         filters_5x5_reduce=16, filters_5x5=32, filters_pool_proj=32)\n",
    "    \n",
    "    x = inception_layer(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192,\n",
    "                         filters_5x5_reduce=32, filters_5x5=96, filters_pool_proj=64)\n",
    "    \n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    \n",
    "    x = inception_layer(x, filters_1x1=192, filters_3x3_reduce=96, filters_3x3=208,\n",
    "                         filters_5x5_reduce=16, filters_5x5=48, filters_pool_proj=64)\n",
    "    \n",
    "    x = inception_layer(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224,\n",
    "                         filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
    "    \n",
    "    x = inception_layer(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256,\n",
    "                         filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
    "    \n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = googlenet()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "9e75016a7061fef0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can adjust the number of epochs as needed.\n",
    "EPOCHS = 30\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,                        \n",
    "    validation_data=validation_dataset\n",
    ")"
   ],
   "id": "14b5dbea75947606",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save training artifacts",
   "id": "62a248c0cd61b9b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model to a file.\n",
    "model.save(os.path.join(os.getcwd(), \"model_googlenet_complex.keras\"))\n",
    "\n",
    "# Save the history to a file.\n",
    "with open(os.path.join(os.getcwd(), \"history_googlenet_complex.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(history, file)"
   ],
   "id": "3ad17b2c847f24c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assess the model's performance",
   "id": "25fdfd45f327f475"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot the accuracy curve",
   "id": "d6d8d573dc0fab44"
  },
  {
   "cell_type": "code",
   "id": "cdb1ca57def88917",
   "metadata": {},
   "source": [
    "def plot_accuracy_curve(training_result, metric):\n",
    "    val_metric = f\"val_{metric}\"\n",
    "    train_perf = training_result.history[metric]\n",
    "    validation_perf = training_result.history[val_metric]\n",
    "    \n",
    "    plt.plot(train_perf, label=metric)\n",
    "    plt.plot(validation_perf, label=val_metric)\n",
    "    \n",
    "    max_val = max(validation_perf)\n",
    "    max_val_epoch = validation_perf.index(max_val)\n",
    "    \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "plot_accuracy_curve(history, \"accuracy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find the epoch at which the difference in training and validation accuracies are minimized.",
   "id": "31de90d2834bc8fc"
  },
  {
   "cell_type": "code",
   "id": "9ca54507-c9a1-4202-b09b-bb1837b4fb97",
   "metadata": {},
   "source": [
    "train_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "acc_diff = [abs(train - val) for train, val in zip(train_acc, val_acc)]\n",
    "\n",
    "min_diff = min(acc_diff)\n",
    "min_diff_epoch = acc_diff.index(min_diff) + 1\n",
    "\n",
    "train_acc_at_min_diff = train_acc[min_diff_epoch - 1]  \n",
    "val_acc_at_min_diff = val_acc[min_diff_epoch - 1]      \n",
    "\n",
    "print(f\"Minimum difference between accuracy and validation accuracy: {min_diff:.1f} at epoch {min_diff_epoch}\")\n",
    "print(f\"Training Accuracy at epoch {min_diff_epoch}: {train_acc_at_min_diff:.1f}\")\n",
    "print(f\"Validation Accuracy at epoch {min_diff_epoch}: {val_acc_at_min_diff:.1f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the model on unseen ImageNet images.\n",
    "\n",
    "### Prepare the evaluation ImageNet images\n",
    "In the same directory as this notebook, you will find a file named `imagenet_224_eval.zip`. This zip file contains the ImageNet images used for evaluating the CNN.\n",
    "Unzip the files to a folder named `imagenet_224_eval`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "imagenet_224_eval\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "9e7baa54c0a65fa9"
  },
  {
   "cell_type": "code",
   "id": "e4bf4f3f-2675-449e-aeae-8259f01d6111",
   "metadata": {},
   "source": [
    "UNSEEN_IMAGENET_IMG_DIR = os.path.join(os.getcwd(), \"imagenet_224_eval\")\n",
    "\n",
    "if not os.path.exists(UNSEEN_IMAGENET_IMG_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{UNSEEN_IMAGENET_IMG_DIR} not found. You need to download the ImageNet evaluation dataset and unzip it into a folder called imagenet_224_eval\")\n",
    "\n",
    "eval_imagenet_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    UNSEEN_IMAGENET_IMG_DIR,          \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE             \n",
    ")\n",
    "\n",
    "# Prefetch for better performance.\n",
    "eval_imagenet_dataset = eval_imagenet_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "loss, accuracy = model.evaluate(eval_imagenet_dataset)\n",
    "\n",
    "print(f\"Loss on unseen ImageNet images: {loss:.1f}\")\n",
    "print(f\"Accuracy on unseen ImageNet images: {accuracy:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the model on CIFAR-10 images.\n",
    "\n",
    "### Prepare the evaluation CIFAR-10 images\n",
    "In the same directory as this notebook, you will find a file named `cifar-10.zip`. This zip file contains the ImageNet images used for evaluating the CNN against a totally different dataset - CIFAR-10.\n",
    "Unzip the files to a folder named `cifar-10`. The unzipped files will have the following structure:\n",
    "\n",
    "```\n",
    "cifar-10\n",
    "----airplane\n",
    "----automobile\n",
    "----...\n",
    "----ship\n",
    "----truck\n",
    "``` \n",
    "If the folder structure doesn't match this, then you will get an error!"
   ],
   "id": "b55f2c9a8960e762"
  },
  {
   "cell_type": "code",
   "id": "f7ca01ed-cbdc-48f5-8783-8d5fc2084b99",
   "metadata": {},
   "source": [
    "CIFAR10_IMG_DIR = os.path.join(os.getcwd(), \"cifar-10\")\n",
    "\n",
    "if not os.path.exists(CIFAR10_IMG_DIR):\n",
    "    raise RuntimeError(\n",
    "        f\"{CIFAR10_IMG_DIR} not found. You need to download the CIFAR-10 dataset and unzip it into a folder called cifar-10\")\n",
    "\n",
    "eval_cifar_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    CIFAR10_IMG_DIR,           \n",
    "    image_size=TARGET_SIZE,      \n",
    "    batch_size=BATCH_SIZE              \n",
    ")\n",
    "\n",
    "# Prefetch for better performance.\n",
    "eval_cifar_dataset = eval_cifar_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "loss, accuracy = model.evaluate(eval_cifar_dataset)\n",
    "\n",
    "print(f\"Loss on CIFAR-10 images: {loss:.1f}\")\n",
    "print(f\"Accuracy CIFAR-10 images: {accuracy:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6873186be39bc23f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
