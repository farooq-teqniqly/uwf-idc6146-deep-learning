{"cells":[{"cell_type":"markdown","source":["The installation is required on Google colab. They are not related to RNN but useful for presenting the code."],"metadata":{"id":"OF3sC9Y2wcWC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGlN5gs8weRH"},"outputs":[],"source":["!pip install setuptools==65.5.0 \"wheel<0.40.0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CHBrF5yxoNu"},"outputs":[],"source":["!pip install d2l==1.0.0b0"]},{"cell_type":"markdown","metadata":{"id":"CMqLFFXXvSPV"},"source":["## Recurrent Neural Network Implementation from Scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WN6z63QPvNdd"},"outputs":[],"source":["%matplotlib inline\n","import math\n","import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","metadata":{"id":"hJTtMW0y4I_B"},"source":["#### RNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-b5NbGQxyeP"},"outputs":[],"source":["class RNNScratch(d2l.Module):\n","    \"\"\"The RNN model implemented from scratch.\"\"\"\n","    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.W_xh = tf.Variable(tf.random.normal(\n","            (num_inputs, num_hiddens)) * sigma)\n","        self.W_hh = tf.Variable(tf.random.normal(\n","            (num_hiddens, num_hiddens)) * sigma)\n","        self.b_h = tf.Variable(tf.zeros(num_hiddens))"]},{"cell_type":"markdown","metadata":{"id":"-zXDA5aEz1B2"},"source":["The forward method below defines how to compute the output and hidden state at any time step, given the current input and the state of the model at the previous time step. Note that the RNN model loops through the outermost dimension of inputs, updating the hidden state one time step at a time. The model here uses a  activation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tHC8czvx9v2"},"outputs":[],"source":["@d2l.add_to_class(RNNScratch)\n","def forward(self, inputs, state=None):\n","    if state is None:\n","        # Initial state with shape: (batch_size, num_hiddens)\n","        state = tf.zeros((inputs.shape[1], self.num_hiddens))\n","    else:\n","        state, = state\n","        state = tf.reshape(state, (-1, self.num_hiddens))\n","    outputs = []\n","    for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n","        state = tf.tanh(tf.matmul(X, self.W_xh) +\n","                         tf.matmul(state, self.W_hh) + self.b_h)\n","        outputs.append(state)\n","    return outputs, state"]},{"cell_type":"markdown","metadata":{"id":"CP_rZUZU0MOg"},"source":["We can feed a minibatch of input sequences into an RNN model as follows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kx7UrG0YyNdX"},"outputs":[],"source":["batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n","rnn = RNNScratch(num_inputs, num_hiddens)\n","X = tf.ones((num_steps, batch_size, num_inputs))\n","outputs, state = rnn(X)"]},{"cell_type":"markdown","metadata":{"id":"gVG1Ufdc0O6H"},"source":["Let’s check whether the RNN model produces results of the correct shapes to ensure that the dimensionality of the hidden state remains unchanged."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X55ROUJyUxl"},"outputs":[],"source":["def check_len(a, n):\n","    \"\"\"Check the length of a list.\"\"\"\n","    assert len(a) == n, f'list\\'s length {len(a)} != expected length {n}'\n","\n","def check_shape(a, shape):\n","    \"\"\"Check the shape of a tensor.\"\"\"\n","    assert a.shape == shape, \\\n","            f'tensor\\'s shape {a.shape} != expected shape {shape}'\n","check_len(outputs, num_steps)\n","check_shape(outputs[0], (batch_size, num_hiddens))\n","check_shape(state, (batch_size, num_hiddens))"]},{"cell_type":"markdown","metadata":{"id":"-kpFyPwa4TqV"},"source":["#### RNN-based Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RBT43hR4SYk"},"outputs":[],"source":["class RNNLMScratch(d2l.Classifier):\n","    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n","    def __init__(self, rnn, vocab_size, lr=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.init_params()\n","\n","    def init_params(self):\n","        self.W_hq = tf.Variable(tf.random.normal(\n","            (self.rnn.num_hiddens, self.vocab_size)) * self.rnn.sigma)\n","        self.b_q = tf.Variable(tf.zeros(self.vocab_size))\n","\n","    def training_step(self, batch):\n","        l = self.loss(self(*batch[:-1]), batch[-1])\n","        self.plot('ppl', tf.exp(l), train=True)\n","        return l\n","\n","    def validation_step(self, batch):\n","        l = self.loss(self(*batch[:-1]), batch[-1])\n","        self.plot('ppl', tf.exp(l), train=False)"]},{"cell_type":"markdown","metadata":{"id":"Z9rwk_OQ5GbN"},"source":["**One-Hot Encoding**\n","\n","Recall that each token is represented by a numerical index indicating the position in the vocabulary of the corresponding word/character/word-piece. You might be tempted to build a neural network with a single input node (at each time step), where the index could be fed in as a scalar value. This works when we are dealing with numerical inputs like price or temperature, where any two values sufficiently close together should be treated similarly. But this does not quite make sense. The 45th and 46th words in our vocabulary happen to be “their” and “said”, whose meanings are not remotely similar.\n","\n","When dealing with such categorical data, the most common strategy is to represent each item by a one-hot encoding. A one-hot encoding is a vector whose length is given by the size of the vocabulary *N*, where all entries are set to 0, except for the entry corresponding to our token, which is set to 1. For example, if the vocabulary had 5 elements, then the one-hot vectors corresponding to indices 0 and 2 would be the following."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnqUD8Bm5Ft6"},"outputs":[],"source":["tf.one_hot(tf.constant([0, 2]), 5)"]},{"cell_type":"markdown","metadata":{"id":"G9XSQOVJ5toQ"},"source":["The minibatches that we sample at each iteration will take the shape (batch size, number of time steps). Once representing each input as a one-hot vector, we can think of each minibatch as a three-dimensional tensor, where the length along the third axis is given by the vocabulary size (`len(vocab)`). We often transpose the input so that we will obtain an output of shape (number of time steps, batch size, vocabulary size). This will allow us to more conveniently loop through the outermost dimension for updating hidden states of a minibatch, time step by time step (e.g., in the above forward method)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq8yAAV75pSr"},"outputs":[],"source":["@d2l.add_to_class(RNNLMScratch)\n","def one_hot(self, X):\n","    # Output shape: (num_steps, batch_size, vocab_size)\n","    return tf.one_hot(tf.transpose(X), self.vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"BlysC9ry56w3"},"source":["**Transforing RNN Outputs**\n","\n","The language model uses a fully connected output layer to transform RNN outputs into token predictions at each time step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uju5ziw96EzT"},"outputs":[],"source":["@d2l.add_to_class(RNNLMScratch)\n","def output_layer(self, rnn_outputs):\n","    outputs = [tf.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n","    return tf.stack(outputs, 1)\n","\n","@d2l.add_to_class(RNNLMScratch)\n","def forward(self, X, state=None):\n","    embs = self.one_hot(X)\n","    rnn_outputs, _ = self.rnn(embs, state)\n","    return self.output_layer(rnn_outputs)"]},{"cell_type":"markdown","metadata":{"id":"e2Eloe4i6VsB"},"source":["Let’s check whether the forward computation produces outputs with the correct shape."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCP2cPg56Xac"},"outputs":[],"source":["model = RNNLMScratch(rnn, num_inputs)\n","outputs = model(tf.ones((batch_size, num_steps), dtype=tf.int64))\n","check_shape(outputs, (batch_size, num_steps, num_inputs))"]},{"cell_type":"markdown","metadata":{"id":"PRaEcjI_7u5P"},"source":["#### Gradient Clipping"]},{"cell_type":"markdown","metadata":{"id":"Qi43FewGF9-_"},"source":["Below we define a method to clip gradients, which is invoked by the `fit_epoch` method of the `d2l.Trainer` class. Note that when computing the gradient norm, we are concatenating all model parameters, treating them as a single giant parameter vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAW0icqqGJSp"},"outputs":[],"source":["@d2l.add_to_class(d2l.Trainer)\n","def clip_gradients(self, grad_clip_val, grads):\n","    grad_clip_val = tf.constant(grad_clip_val, dtype=tf.float32)\n","    new_grads = [tf.convert_to_tensor(grad) if isinstance(\n","        grad, tf.IndexedSlices) else grad for grad in grads]\n","    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)) for grad in new_grads))\n","    if tf.greater(norm, grad_clip_val):\n","        for i, grad in enumerate(new_grads):\n","            new_grads[i] = grad * grad_clip_val / norm\n","        return new_grads\n","    return grads"]},{"cell_type":"markdown","metadata":{"id":"kVrTESaFGQBH"},"source":["#### Training"]},{"cell_type":"markdown","metadata":{"id":"dIZbg4f7GVOs"},"source":["Using *The Time Machine* dataset (`data`), we train a character-level language model (`model`) based on the RNN (`rnn`) implemented from scratch. Note that we first calculate the gradients, then clip them, and finally update the model parameters using the clipped gradients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB5oBJPqGevz"},"outputs":[],"source":["data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n","with d2l.try_gpu():\n","    rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n","    model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n","trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","metadata":{"id":"zbCk1XOnGluc"},"source":["#### Decoding"]},{"cell_type":"markdown","metadata":{"id":"ijhiS2vBGoQq"},"source":["Once a language model has been learned, we can use it not only to predict the next token but to continue predicting each subsequent token, treating the previously predicted token as though it were the next token in the input. Sometimes we will just want to generate text as though we were starting at the beginning of a document. However, it is often useful to condition the language model on a user-supplied prefix. For example, if we were developing an autocomplete feature for search engine or to assist users in writing emails, we would want to feed in what they had written so far (the prefix), and then generate a likely continuation.\n","\n","The following `predict` method generates a continuation, one character at a time, after ingesting a user-provided `prefix`, When looping through the characters in `prefix`, we keep passing the hidden state to the next time step but do not generate any output. This is called the *warm-up* period. After ingesting the prefix, we are now ready to begin emitting the subsequent characters, each of which will be fed back into the model as the input at the subsequent time step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFM9X_oQG3g9"},"outputs":[],"source":["@d2l.add_to_class(RNNLMScratch)\n","def predict(self, prefix, num_preds, vocab, device=None):\n","    state, outputs = None, [vocab[prefix[0]]]\n","    for i in range(len(prefix) + num_preds - 1):\n","        X = tf.constant([[outputs[-1]]])\n","        embs = self.one_hot(X)\n","        rnn_outputs, state = self.rnn(embs, state)\n","        if i < len(prefix) - 1:  # Warm-up period\n","            outputs.append(vocab[prefix[i + 1]])\n","        else:  # Predict num_preds steps\n","            Y = self.output_layer(rnn_outputs)\n","            outputs.append(int(tf.reshape(tf.argmax(Y, axis=2), 1)))\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])"]},{"cell_type":"markdown","metadata":{"id":"TzSk35-2HBo1"},"source":["In the following, we specify the prefix and have it generate 20 additional characters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ry2540QXG_2Q"},"outputs":[],"source":["model.predict('it has', 20, data.vocab)"]},{"cell_type":"markdown","metadata":{"id":"GGdPocWXHKIV"},"source":["While implementing the above RNN model from scratch is instructive, it is not convenient. In the next section, we will see how to leverage deep learning frameworks to whip up RNNs using standard architectures, and to reap performance gains by relying on highly optimized library functions."]},{"cell_type":"markdown","metadata":{"id":"FJdFBhSy1ufr"},"source":["## Concise Implementation of Recurrent Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXx9egVs2Fpn"},"outputs":[],"source":["import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","metadata":{"id":"3s1Vb6MP2Zqe"},"source":["#### Define the Model"]},{"cell_type":"markdown","metadata":{"id":"4qhf1rsK21DY"},"source":["We define the following class using the RNN implemented by high-level APIs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjCVMpSo2Nww"},"outputs":[],"source":["class RNN(d2l.Module):\n","    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n","    def __init__(self, num_hiddens):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.rnn = tf.keras.layers.SimpleRNN(\n","            num_hiddens, return_sequences=True, return_state=True,\n","            time_major=True)\n","\n","    def forward(self, inputs, H=None):\n","        outputs, H = self.rnn(inputs, H)\n","        return outputs, H"]},{"cell_type":"markdown","metadata":{"id":"eaIMyAuB2uUZ"},"source":["The following RNNLM class defines a complete RNN-based language model. Note that we need to create a separate fully connected output layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8DWNnkN2rKh"},"outputs":[],"source":["class RNNLM(d2l.RNNLMScratch):\n","    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n","    def init_params(self):\n","        self.linear = tf.keras.layers.Dense(self.vocab_size)\n","\n","    def output_layer(self, hiddens):\n","        return tf.transpose(self.linear(hiddens), (1, 0, 2))"]},{"cell_type":"markdown","metadata":{"id":"I47JXOH13Ah7"},"source":["#### Training and Predicting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Su8Ez1am3EaY"},"outputs":[],"source":["data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n","rnn = RNN(num_hiddens=32)\n","model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\n","model.predict('it has', 20, data.vocab)"]},{"cell_type":"markdown","metadata":{"id":"P4o20F4N3JC7"},"source":["Next, we train our model, leveraging the high-level API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6db7tIz53Kwb"},"outputs":[],"source":["with d2l.try_gpu():\n","    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","metadata":{"id":"d3dENqqfHRd2"},"source":["Compared with *RNN Implementation from Scratch*, this model achieves comparable perplexity, but runs faster due to the optimized implementations. As before, we can generate predicted tokens following the specified prefix string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYJlAE-D38TE"},"outputs":[],"source":["model.predict('it has', 20, data.vocab)"]},{"cell_type":"markdown","metadata":{"id":"v6ndGCsAHiEX"},"source":["High-level APIs in deep learning frameworks provide implementations of standard RNNs. These libraries help you to avoid wasting time reimplementing standard models. Moreover, framework implementations are often highly optimized, leading to significant (computational) performance gains as compared to implementations from scratch."]}],"metadata":{"colab":{"collapsed_sections":["-kpFyPwa4TqV"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}